{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritamgh/DLT-lab/blob/main/Week12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWmegUmnEdgW",
        "outputId": "2d4c7e34-9ef4-4d40-b4a6-899f81d806f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator params: 1.07M\n",
            "Discriminator params: 0.66M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | lossD=0.529 | lossG=2.940 | saved /content/dcgan_samples/epoch_001.png\n",
            "Epoch 2/10 | lossD=0.723 | lossG=3.075 | saved /content/dcgan_samples/epoch_002.png\n",
            "Epoch 3/10 | lossD=0.845 | lossG=2.977 | saved /content/dcgan_samples/epoch_003.png\n",
            "Epoch 4/10 | lossD=0.568 | lossG=2.535 | saved /content/dcgan_samples/epoch_004.png\n",
            "Epoch 5/10 | lossD=0.532 | lossG=1.974 | saved /content/dcgan_samples/epoch_005.png\n",
            "Epoch 6/10 | lossD=1.113 | lossG=1.498 | saved /content/dcgan_samples/epoch_006.png\n",
            "Epoch 7/10 | lossD=0.574 | lossG=2.339 | saved /content/dcgan_samples/epoch_007.png\n",
            "Epoch 8/10 | lossD=0.789 | lossG=2.108 | saved /content/dcgan_samples/epoch_008.png\n",
            "Epoch 9/10 | lossD=0.797 | lossG=2.860 | saved /content/dcgan_samples/epoch_009.png\n",
            "Epoch 10/10 | lossD=0.938 | lossG=0.809 | saved /content/dcgan_samples/epoch_010.png\n",
            "Done. Sample images saved in: /content/dcgan_samples\n"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# Lab 12: DCGAN on CIFAR-10\n",
        "# =======================\n",
        "!pip -q install torch torchvision torchaudio torchinfo\n",
        "\n",
        "import os, math, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- Config\n",
        "batch_size = 128\n",
        "image_size = 32\n",
        "nz = 100       # latent dim\n",
        "ngf = 64       # generator feature maps\n",
        "ndf = 64       # discriminator feature maps\n",
        "nc = 3         # RGB\n",
        "epochs = 10    # bump to 50+ for better results\n",
        "out_dir = \"/content/dcgan_samples\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# --- Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),  # [-1,1]\n",
        "])\n",
        "trainset = datasets.CIFAR10(root=\"/content/data\", train=True, download=True, transform=transform)\n",
        "loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# --- Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # input Z: (nz) -> (ngf*4) x 4 x 4\n",
        "            nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "            # -> (ngf*2) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "            # -> (ngf) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # -> (nc) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z): return self.net(z)\n",
        "\n",
        "# --- Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # (nc) x 32 x 32 -> (ndf) x 16 x 16\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # -> (ndf*2) x 8 x 8\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # -> (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # -> 1 x 1 x 1\n",
        "            nn.Conv2d(ndf*4, 1, 4, 1, 0, bias=False)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).view(-1)\n",
        "\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "print(f\"Generator params: {sum(p.numel() for p in G.parameters())/1e6:.2f}M\")\n",
        "print(f\"Discriminator params: {sum(p.numel() for p in D.parameters())/1e6:.2f}M\")\n",
        "\n",
        "# --- Loss/Opt\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optG = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "optD = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# --- Training\n",
        "fixed_z = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "step = 0\n",
        "for epoch in range(1, epochs+1):\n",
        "    for real, _ in loader:\n",
        "        real = real.to(device)\n",
        "        b = real.size(0)\n",
        "        # Real/Fake labels (with a tiny smoothing)\n",
        "        real_lbl = torch.ones(b, device=device) * 0.9\n",
        "        fake_lbl = torch.zeros(b, device=device)\n",
        "\n",
        "        # ---- Train D\n",
        "        D.zero_grad()\n",
        "        out_real = D(real)\n",
        "        loss_real = criterion(out_real, real_lbl)\n",
        "\n",
        "        z = torch.randn(b, nz, 1, 1, device=device)\n",
        "        fake = G(z).detach()\n",
        "        out_fake = D(fake)\n",
        "        loss_fake = criterion(out_fake, fake_lbl)\n",
        "\n",
        "        lossD = loss_real + loss_fake\n",
        "        lossD.backward()\n",
        "        optD.step()\n",
        "\n",
        "        # ---- Train G\n",
        "        G.zero_grad()\n",
        "        z = torch.randn(b, nz, 1, 1, device=device)\n",
        "        gen = G(z)\n",
        "        out = D(gen)\n",
        "        lossG = criterion(out, real_lbl)   # want D to call fakes \"real\"\n",
        "        lossG.backward()\n",
        "        optG.step()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    # Save sample grid each epoch\n",
        "    with torch.no_grad():\n",
        "        samples = G(fixed_z).cpu()\n",
        "        samples = (samples+1)/2  # back to [0,1]\n",
        "        utils.save_image(samples, f\"{out_dir}/epoch_{epoch:03d}.png\", nrow=8)\n",
        "    print(f\"Epoch {epoch}/{epochs} | lossD={lossD.item():.3f} | lossG={lossG.item():.3f} | saved {out_dir}/epoch_{epoch:03d}.png\")\n",
        "\n",
        "print(\"Done. Sample images saved in:\", out_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHY5NhAFzRNFFr7GNa0GhG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}